# ant 实验解读手册（每个实验做什么、怎么做、关键细节）

这份文档专门解读 `ant/experiments/` 里的实验脚本。  
重点回答 5 个问题：

1. 这个实验要验证什么？
2. 输入数据是什么？
3. 核心步骤怎么做？
4. 输出结果怎么读？
5. 有什么常见坑和改进点？

---

## 0. 全局实验链路（先看这个）

项目主线是：  
**先证明问题存在 -> 构建训练/评测数据 -> 训练约束编码器 -> 线上双视角检索 -> 多指标对比 -> 错误与延迟分析**。

对应脚本链路（推荐顺序）：

1. `poc_negation_gap.py`
2. `build_triplets.py`
3. `build_constraint_benchmark.py`
4. `train_constraint_encoder.py`
5. `eval_constraint_encoder.py`
6. `retrieve_then_filter.py`
7. `rag_eval.py`
8. `rag_grid_search.py`
9. `build_retrieval_benchmark.py`
10. `eval_retrieval_metrics.py`
11. `run_bm25_baseline.py`
12. `run_cross_encoder_baseline.py`
13. `rag_category_report.py`
14. `rag_e2e_eval.py`
15. `error_analysis_report.py`
16. `latency_benchmark.py`
17. `render_category_table.py`

`common.py` 与 `metrics.py` 是实验基建，不是单独实验，但所有实验都依赖它们。

---

## 1) PoC：`poc_negation_gap.py`

### 做什么
验证“普通语义向量”在否定/矛盾语义上区分能力不足（项目立项依据）。

### 怎么做
- 从 SNLI 抽取 `(premise, entailment, contradiction)` 三元关系。
- 同一个 `premise` 分别与 entailment/contradiction 算余弦相似度。
- 看 `margin = sim(entailment) - sim(contradiction)` 分布。

### 关键参数
- `--dataset`, `--split`, `--max-samples`
- `--model`（默认 MiniLM）

### 输出
- `outputs/reports/poc_negation_gap_summary.txt`
- `outputs/reports/poc_negation_gap_scores.csv`
- `outputs/figures/poc_negation_gap_hist.png`

### 怎么读结果
- `pairwise accuracy (ent > contra)` 越高越好。
- 若 contradiction 分数常偏高，说明“只靠 topic encoder 不够”。

### 常见坑
- 只看均值会掩盖长尾问题，建议看直方图重叠区间。

---

## 2) 构建 triplet 与 smoke：`build_triplets.py`

### 做什么
生成训练约束编码器的三元组数据，并给出最小可运行 smoke 集。

### 怎么做
- 从 SNLI 里按 `premise` 聚合，找同时有 entailment 和 contradiction 的样本。
- 组装为：
  - `query = premise`
  - `positive = entailment`
  - `hard_negative = contradiction`
- 另外生成：
  - `smoke_eval.jsonl`（小规模约束查询）
  - `demo_corpus.jsonl`（去重语料）

### 输出
- `data/processed/train_triplets.jsonl`
- `data/processed/val_triplets.jsonl`
- `data/processed/smoke_eval.jsonl`
- `data/processed/demo_corpus.jsonl`

### 怎么读结果
- 先看样本量够不够（train/val 是否达上限）。
- spot check 若干 triplet，确认负样本是“矛盾”而非随机负例。

### 常见坑
- 标签语义依赖 SNLI 映射，若换数据集需确认 label 编码一致。

---

## 3) 构建大评测集：`build_constraint_benchmark.py`

### 做什么
合成更大规模约束检索评测数据（按类别覆盖）。

### 怎么做
按三类模板生成 query+docs：
- `negation`：如 `not dirty`
- `exclusion`：如 `without peanuts`
- `numeric`：如 `under 100 dollars`

每条 query 绑定多条 doc，并标注 `satisfies: 0/1`。

### 关键参数
- `--num-negation`, `--num-exclusion`, `--num-numeric`
- `--seed`

### 输出
- 默认 `data/processed/constraint_benchmark_v1.jsonl`

### 怎么读结果
- 看三类数量是否平衡。
- 抽查模板文本是否过于重复（过重复会高估模型效果）。

### 常见坑
- 纯模板数据容易“任务过干净”，对真实查询泛化有限。

---

## 4) 训练约束编码器：`train_constraint_encoder.py`

### 做什么
训练专门识别“约束满足关系”的句向量编码器。

### 怎么做
- 把 triplet 转 `InputExample(texts=[q,p,n])`
- 模型结构：`Transformer + Pooling`
- 损失：`MultipleNegativesRankingLoss`
- 目标：`sim(q,p) > sim(q,n)`

### 关键参数
- `--base-model`（默认 `distilbert-base-uncased`）
- `--epochs`, `--batch-size`, `--max-seq-len`
- `--train-file`, `--output-dir`

### 输出
- 模型目录：`outputs/checkpoints/constraint-encoder-v1/`

### 怎么读结果
- 训练阶段主要看 loss 下降与是否正常收敛。
- 更关键是下一步离线评估是否优于 baseline。

### 常见坑
- batch 太大易 OOM；先调小 batch 再考虑梯度累计。

---

## 5) 编码器离线评估：`eval_constraint_encoder.py`

### 做什么
比较“训练前 baseline encoder”与“训练后 constraint encoder”的 pairwise 能力。

### 怎么做
- 对 val triplets 分别计算：
  - `pos_scores = sim(q, positive)`
  - `neg_scores = sim(q, hard_negative)`
- 统计：
  - `accuracy`（正分 > 负分比例）
  - `avg_margin`, `median_margin`

### 输出
- `outputs/reports/constraint_eval_report.json`

### 怎么读结果
- 看 `constraint.accuracy` 是否高于 `baseline.accuracy`。
- `avg_margin_abs` 是否明显正向增长。

### 常见坑
- 只看 accuracy 不看 margin，可能漏掉“置信度提升”信息。

---

## 6) 在线检索演示：`retrieve_then_filter.py`

### 做什么
验证双视角检索在单 query 上能否把约束文档顶上来。

### 怎么做
1. topic encoder 在全语料初召回
2. constraint encoder 对候选打分
3. 可选 `tau` 过滤不满足约束候选
4. 融合分重排：`alpha*t + (1-alpha)*c`

### 关键参数
- `--alpha`：主题 vs 约束权重
- `--tau`：约束阈值（为空则不做过滤）
- `--top-k-retrieve`, `--top-k-final`

### 输出
- 终端直接打印每条结果的 `final/topic/constraint` 分数和文本。

### 怎么读结果
- 看 top1/top3 是否明显更符合约束。
- 比较 `tau` 开启与关闭时结果变化。

### 常见坑
- `tau` 太高会过滤空候选，需配回退策略（脚本里已有）。

---

## 7) Smoke CCR：`rag_eval.py`

### 做什么
在小规模 smoke 数据上快速验证 dual 是否优于 vanilla。

### 怎么做
- 对每 query 计算 vanilla 排名（topic only）
- dual 排名（topic+constraint）
- 计算 `CCR@k` 并求平均

### 输出
- `outputs/reports/rag_eval_report.json`

### 怎么读结果
- 看 `avg_dual_ccr - avg_vanilla_ccr` 是否为正。
- 检查 `per_query`，避免“均值好看但某类严重退化”。

### 常见坑
- smoke 集很小，只能做 sanity check，不是最终结论。

---

## 8) 网格搜索：`rag_grid_search.py`

### 做什么
系统寻找最优 `(alpha, tau)` 组合。

### 怎么做
- 遍历参数网格；
- 每组参数计算全 query 平均 `CCR@k`；
- 记录最优组合和相对 vanilla 改进。

### 关键参数
- `--alphas=...`
- `--taus=...`
- `--top-k`
- `--max-queries`（可抽样提速）

### 输出
- `outputs/reports/rag_grid_search_*.json`

### 怎么读结果
- 看 `best` 与 `best_improvement`。
- 同时看 `all_results` 前几名是否稳定，不要只信单点最优。

### 常见坑
- 在小样本上调参会过拟合，建议固定验证集口径。

---

## 9) 构建统一检索评测口径：`build_retrieval_benchmark.py`

### 做什么
把 query-doc 格式转换为“标准检索评测格式”。

### 怎么做
- 全局去重文档文本，生成统一 `doc_id` 语料库。
- 每个 query 生成：
  - `topical_relevant_doc_ids`
  - `constraint_satisfying_doc_ids`
  - `graded_relevance`

### 输出
- `data/processed/retrieval_corpus_v1.jsonl`
- `data/processed/retrieval_benchmark_v1.jsonl`

### 怎么读结果
- 检查每个 query 的 relevant 集合非空。
- 确认 `constraint_satisfying_doc_ids` 子集逻辑合理。

### 常见坑
- 若源数据中同文本多语义，去重可能引入标签冲突，需留意。

---

## 10) 主评测：`eval_retrieval_metrics.py`

### 做什么
在统一 benchmark 上比较 vanilla 与 dual 的多指标表现。

### 怎么做
- 预编码全语料 topic/constraint 向量（提速）
- 每 query 计算：
  - vanilla 排名（topic）
  - dual 排名（候选召回 + 约束阈值 + 融合重排 + 全量回填）
- 指标：
  - `Recall@10`, `Recall@100`
  - `NDCG@10`
  - `CCR@10`
- 输出 overall / by_category / per_query

### 关键参数
- `--retrieve-k`：dual 候选深度
- `--alpha`, `--tau`

### 输出
- `outputs/reports/retrieval_metrics_*.json`

### 怎么读结果
- 先看 overall 里 dual 是否在 `CCR@10` 提升。
- 再看 recall 与 ndcg 是否可接受（不要只追 CCR）。
- 最后看 `by_category` 找收益来源与退化类别。

### 常见坑
- `tau` 太激进可能牺牲 recall。
- 需关注“CCR 提升是否以召回明显下降为代价”。

---

## 11) BM25 基线：`run_bm25_baseline.py`

### 做什么
给出词法检索基线，避免只与 embedding 家族比较。

### 怎么做
- 手写 BM25（tokenize / tf / df / idf）
- 对 benchmark 每 query 排序并算同口径指标

### 输出
- `outputs/reports/bm25_metrics_report.json`

### 怎么读结果
- 看 lexical baseline 在 numeric/exclusion 上是否有意外优势。
- 对比 dual 的改进是否真正有价值。

### 常见坑
- token 规则简单，遇到复杂文本会偏弱。

---

## 12) Cross-Encoder 基线：`run_cross_encoder_baseline.py`

### 做什么
提供更强 reranker 基线，提升说服力。

### 怎么做
1. 用 topic encoder 召回 top-k 候选
2. Cross-Encoder 对 `(query, doc)` 打分重排
3. 为保持指标可算，剩余文档按 topic 排序补齐

### 细节亮点
- 兼容 cross 输出二维 logits 的场景，会尝试找 entailment 维度。
- 记录延迟统计（p50/p95/mean）。

### 输出
- `outputs/reports/cross_encoder_metrics_report.json`

### 怎么读结果
- 看是否在 `NDCG@10` 明显更强。
- 同时比较 `CCR@10` 与延迟代价。

### 常见坑
- 候选深度 `candidate-k` 太小会压制上限，太大延迟飙升。

---

## 13) 分类报告：`rag_category_report.py`

### 做什么
按 `negation/exclusion/numeric` 分开看 dual 提升，解释“哪里有效”。

### 怎么做
- 同时计算 vanilla 和 dual 的 `CCR@k`
- 输出每类 count、均值和 improvement

### 输出
- `outputs/reports/rag_category_report_*.json`

### 怎么读结果
- 看提升是否集中在 negation（通常是约束编码器最直接收益点）。
- 若 numeric 退化，常是阈值或模板偏差问题。

---

## 14) Proxy 端到端：`rag_e2e_eval.py`

### 做什么
从检索结果近似估计“回答可信度”。

### 怎么做
- 从 `eval_retrieval_metrics` 的 `per_query` 读取 top10。
- 代理规则：top1 若满足约束，则记作 faithful。
- 汇总：
  - `proxy_faithfulness`
  - `negation_failure_rate`
  - 按类别 faithfulness

### 输出
- `outputs/reports/rag_e2e_proxy_*.json`

### 怎么读结果
- 这是 proxy，不是完整 LLM 生成评测。
- 可用于快速比较方案，不适合作为最终论文端到端结论。

---

## 15) 错误案例：`error_analysis_report.py`

### 做什么
自动抽取“dual 表现差或失败”的案例并归因分类。

### 怎么做
- 从 retrieval report 读取每 query 结果
- 根据 `delta_ccr`、top1 是否满足约束、类别等规则打标签
- 输出最差若干案例到 markdown

### 输出
- `outputs/reports/error_analysis_cases.md`

### 怎么读结果
- 看失败类型分布（negation-scope / numeric-threshold 等）
- 看 vanilla top1 vs dual top1 文本差异，定位策略问题

### 常见坑
- 规则归因是启发式，不等于严格因果结论。

---

## 16) 延迟评测：`latency_benchmark.py`

### 做什么
比较不同方案推理时延，回答“效果提升值不值得”。

### 怎么做
- 方案：`vanilla`, `dual`, `bm25`, `cross(optional)`
- 指标：`mean/p50/p95` 毫秒
- dual 计时包含 topic+constraint 编码与候选处理

### 输出
- `outputs/reports/latency_benchmark_report*.json`

### 怎么读结果
- 对比 `CCR 提升` 与 `p95 增幅`，做性价比判断。
- 线上场景更应看 p95，而不是只看平均值。

---

## 17) 表格渲染：`render_category_table.py`

### 做什么
把 category JSON 报告转为可直接贴文档/论文的 markdown 表格。

### 怎么做
- 读取一个或两个 category report
- 统一格式输出（count、vanilla、dual、improvement）

### 输出
- 自定义 `--output-md` 路径

### 怎么读结果
- 这是展示层工具，核心数据仍以 JSON 为准。

---

## 18) 基建脚本（支撑所有实验）

## `common.py`
- 项目路径常量
- `read_jsonl / write_jsonl`
- `pairwise_accuracy`
- `load_sentence_encoder`（含本地模型 pad token 兼容）

## `metrics.py`
- `recall_at_k`
- `precision_at_k`
- `ccr_at_k`（本质是对约束满足标签做 precision@k）
- `ndcg_at_k`

---

## 19) 你现在可以用的“实验阅读模板”

后面你看任意一个实验，都可以按这 6 行来理解：

1. **问题**：它要解决哪个实验问题？
2. **输入**：读取哪个文件，字段是什么？
3. **方法**：排序/过滤/融合/训练的核心一步是什么？
4. **指标**：输出用什么指标度量？
5. **结论**：与哪个 baseline 比较？
6. **风险**：结果可能被什么数据或参数偏置影响？

---

## 20) 如果你要继续，我建议的下一步

你现在已经有：
- `代码完整解读.md`（全景）
- `代码逐行注释版.md`（逐行）
- 本文 `实验解读手册.md`（实验视角）

下一步我可以再给你补一份：  
**“实验结果阅读模板 + 你当前各报告的实战解读”**，把 `outputs/reports/*.json` 逐个翻译成人话结论（可直接用于写作）。

