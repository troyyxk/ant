# ant 项目代码完整解读（学习版）

这份文档是给“暂时不熟代码”的同学准备的，目标是帮助你看懂 `/home/xingkun/ant` 中整个实验管线：  
**数据构建 -> 模型训练 -> 双视角检索 -> 多指标评估 -> 基线对比 -> 错误分析**。

---

## 1. 项目在做什么（先建立全局图）

这个项目实现的是 **Constraint-Aware RAG（约束感知检索）**，核心思想：

- `Topic Encoder`：负责主题相关性（像普通语义检索）
- `Constraint Encoder`：负责约束满足性（如 not dirty / 无花生 / 价格上限）
- 在线时把两者融合：先召回，再按约束过滤或重排

一句话：  
**不是只找“相关”的文档，而是要找“相关且满足约束”的文档。**

---

## 2. 代码目录与职责

你当前主要要看的是 `experiments/` 目录里的 19 个 Python 脚本：

### A. 通用基础
- `experiments/common.py`：路径常量、jsonl 读写、随机种子、模型加载、pairwise 指标
- `experiments/metrics.py`：`Recall@k` / `NDCG@k` / `CCR@k`

### B. 数据构建
- `experiments/poc_negation_gap.py`：先做 PoC，验证普通 embedding 对矛盾/否定区分不够好
- `experiments/build_triplets.py`：从 SNLI 构建训练三元组 + smoke 测试数据
- `experiments/build_constraint_benchmark.py`：构建更大的合成约束评测集
- `experiments/build_retrieval_benchmark.py`：把 query-doc 数据整理成统一检索 benchmark/corpus

### C. 训练与离线能力验证
- `experiments/train_constraint_encoder.py`：训练约束编码器
- `experiments/eval_constraint_encoder.py`：对比 baseline encoder 与 constraint encoder（pairwise）

### D. 检索与评测主线
- `experiments/retrieve_then_filter.py`：单条查询演示（检索 + 约束重排）
- `experiments/rag_eval.py`：smoke 集上的 CCR 评估
- `experiments/rag_grid_search.py`：`alpha/tau` 网格搜索
- `experiments/rag_category_report.py`：按类别出分（negation/exclusion/numeric）
- `experiments/eval_retrieval_metrics.py`：正式多指标评估（Recall/NDCG/CCR）
- `experiments/rag_e2e_eval.py`：从检索报告推导 proxy 端到端 faithfulness

### E. 基线与效率
- `experiments/run_bm25_baseline.py`：BM25 基线
- `experiments/run_cross_encoder_baseline.py`：Cross-Encoder 重排基线
- `experiments/latency_benchmark.py`：延迟对比（vanilla/dual/bm25/cross）

### F. 报告与分析
- `experiments/error_analysis_report.py`：错误案例归因并输出 markdown
- `experiments/render_category_table.py`：把 category JSON 渲染成论文风格表格 markdown

---

## 3. 先懂两个核心参数：`alpha` 和 `tau`

在多个脚本中都会看到：

- `final_score = alpha * topic_score + (1 - alpha) * constraint_score`
- `tau`：约束分数阈值，`constraint_score >= tau` 才保留（否则被过滤）

直觉：

- `alpha` 越大：更偏“主题相关性”
- `alpha` 越小：更偏“约束满足”
- `tau` 越高：过滤更严格，通常 CCR 提升但可能伤害召回

---

## 4. 数据格式（你看代码时最该先认清）

### 4.1 Triplet 训练样本（`build_triplets.py`）
- 字段：`query`, `positive`, `hard_negative`
- 用于训练 `Constraint Encoder`

### 4.2 约束评测样本（`build_constraint_benchmark.py`）
- 字段：`query`, `docs`, `category`
- `docs` 内每条有 `text`, `satisfies(0/1)`

### 4.3 统一检索 benchmark（`build_retrieval_benchmark.py`）
- `retrieval_corpus_v1.jsonl`：全局去重后的文档池（`doc_id`, `text`）
- `retrieval_benchmark_v1.jsonl`：每个 query 的相关集与约束满足集
  - `topical_relevant_doc_ids`
  - `constraint_satisfying_doc_ids`
  - `graded_relevance`

---

## 5. 按执行顺序解读每段代码在干嘛

## 5.1 `poc_negation_gap.py`（为什么要做这个项目）

作用：先证明“普通 embedding 容易把矛盾句也当成相似句”。  
做法：
1. 从 SNLI 取 `(premise, entailment, contradiction)`
2. 分别算余弦相似度
3. 统计 `entailment_score - contradiction_score` 的 margin 分布

产出：
- `outputs/reports/poc_negation_gap_summary.txt`
- `outputs/reports/poc_negation_gap_scores.csv`
- `outputs/figures/poc_negation_gap_hist.png`

如果这里 margin 不够大，说明仅靠主题 embedding 不够，需要额外的约束建模。

## 5.2 `build_triplets.py`（训练数据准备）

关键逻辑：
- 从 SNLI 中按 `premise` 聚合，找到同时含 entailment 与 contradiction 的样本
- 组成三元组：
  - `query = premise`
  - `positive = entailment hypothesis`
  - `hard_negative = contradiction hypothesis`

同时生成：
- `smoke_eval.jsonl`（小规模手工查询集合）
- `demo_corpus.jsonl`（演示用语料池）

## 5.3 `train_constraint_encoder.py`（约束编码器训练）

模型结构：
- `Transformer(base-model)` + `Pooling`
- 用 `SentenceTransformer` 训练
- loss 是 `MultipleNegativesRankingLoss`

训练目标：让 `query` 更接近 `positive`，远离 `hard_negative`。

## 5.4 `eval_constraint_encoder.py`（离线 pairwise 验证）

同一批 val triplets 上比较：
- baseline model（默认 MiniLM）
- constraint model（你训练好的模型）

输出重点：
- `accuracy`：`sim(q,p) > sim(q,n)` 的比例
- `avg_margin`：平均间隔

这一步是“编码器本体能力”的 sanity check。

## 5.5 `retrieve_then_filter.py`（在线单查询演示）

流程：
1. Topic 模型对全库打分，取前 `top-k-retrieve`
2. Constraint 模型给候选集打分
3. 可选 `tau` 过滤
4. 融合分数重排，输出前 `top-k-final`

这就是你项目里“Retrieve -> Filter/Rerank”的最小落地版本。

## 5.6 `rag_eval.py`（smoke 的 CCR）

对每个 query 比较：
- vanilla（只看 topic）
- dual（topic + constraint）

指标是 `CCR@k`：top-k 中满足约束文档的比例。  
这是最早期“是否有效”的快速验证脚本。

## 5.7 `rag_grid_search.py`（调参主脚本）

遍历：
- `alpha` 网格
- `tau` 网格

每组参数都算平均 CCR，最后给出 best 配置和相对 vanilla 的改进。  
这是你后续大量报告（如 `alpha_sweep_tau_x`）的来源。

## 5.8 `build_constraint_benchmark.py` + `build_retrieval_benchmark.py`

这两个连起来解决“评测口径”问题：

- 前者构造 query + docs + satisfies 标签（含 `negation/exclusion/numeric`）
- 后者把数据转成“检索评测友好结构”（全局 `doc_id`，每 query 的 relevance 集合）

有了它们，后续就能做标准检索指标而不仅是 CCR。

## 5.9 `eval_retrieval_metrics.py`（主评测脚本，最关键）

这是目前最完整评测：
- `Recall@10/100`
- `NDCG@10`
- `CCR@10`
- 同时输出 overall、按类别、逐 query 明细

dual 排序逻辑：
1. 用 topic 做初召回（`retrieve_k`）
2. 用 constraint 打分并按 `tau` 过滤
3. 候选内按融合分排序
4. 为了完整排名，把剩余文档按 vanilla 顺序接回去

这个设计保证了 `recall@100` 等指标可计算。

## 5.10 `rag_e2e_eval.py`（proxy 端到端）

它不跑 LLM 生成，而是从检索结果做代理判断：
- 规则：若 top1 满足约束，视作“faithful”
- 得到 `proxy_faithfulness` 和 `negation_failure_rate`

注意：这是 proxy，不是严格 QA 端到端评测。

## 5.11 基线：`run_bm25_baseline.py` 与 `run_cross_encoder_baseline.py`

### BM25
- 纯词法方法，手写 BM25 实现
- 在同一 benchmark 上出同样 4 个指标，便于横向对比

### Cross-Encoder
- 先 topic 召回 top-k 候选，再用 cross-encoder 重排
- 若输出是多分类 logits，会尝试识别 entailment 维度
- 同时记录延迟统计（p50/p95/mean）

## 5.12 诊断与展示：`latency_benchmark.py` / `error_analysis_report.py` / `render_category_table.py`

- `latency_benchmark.py`：统一测各方法延迟（含可选 cross）
- `error_analysis_report.py`：找最差案例并分类（negation-scope / numeric-threshold 等）
- `render_category_table.py`：把 category 报告转 markdown 表，适合论文插表

---

## 6. 你读代码时可重点抓的“5 个关键函数/逻辑”

1. `common.load_sentence_encoder`  
   解决本地 causal LM 没有 `pad_token` 的兼容问题（否则批量编码会出错）

2. `metrics.ndcg_at_k`  
   明确了本项目如何做 graded relevance（虽然当前多数是二值）

3. `eval_retrieval_metrics.py` 的 dual rank 构造  
   有过滤、有融合、有回填，口径最完整

4. `rag_grid_search.py` 的 `keep_idx` 逻辑  
   `tau` 过滤后若为空，会回退到 topic 排序，避免空结果

5. `run_cross_encoder_baseline.py` 的多输出处理  
   对不同 cross-encoder 输出形状做兼容，避免指标报错

---

## 7. 给你的“最省力学习顺序”（建议按这个跑）

1. 先看 `readme.md`（理解整体目标）  
2. 看 `common.py` + `metrics.py`（打底）  
3. 看 `build_triplets.py`（数据从哪里来）  
4. 看 `train_constraint_encoder.py` + `eval_constraint_encoder.py`（模型怎么训练和验证）  
5. 看 `retrieve_then_filter.py`（线上逻辑最直观）  
6. 看 `rag_grid_search.py` + `eval_retrieval_metrics.py`（论文级评测核心）  
7. 最后看基线/延迟/错误分析脚本

---

## 8. 一些你现在就能理解的“工程判断”

- 这套代码已经形成完整实验闭环，不是零散脚本
- 指标设计已经从单一 CCR 走向多指标（Recall/NDCG/CCR）
- 最关键价值是：把“语义相关”与“约束满足”拆成双视角，并可控融合
- 目前端到端仍是 proxy faithfulness，不是完整生成评测

---

## 9. 常见困惑（你后续读代码时大概率会遇到）

- 为什么很多地方要同时存 `top10_doc_ids` 和 `constraint_satisfying_doc_ids`？  
  为了后续错误分析与 proxy e2e，不只是算一个均值指标。

- 为什么 dual 排名后还要拼接剩余文档？  
  为了保证排名全长，`Recall@100` 等大 k 指标才能一致比较。

- `alpha=0` 是不是只看约束？  
  在融合分上是，但仍可能先经过 topic 召回候选（取决于脚本）。

- `tau=-1` 有什么意义？  
  近似“不做过滤”，只做加权重排，常用于和强过滤方案比较。

---

## 10. 你可以怎么继续学（下一步）

如果你愿意，我可以在这份文档基础上继续给你补一版：

1. **逐行版**：按每个脚本的重要代码段加“中文注释解释”  
2. **运行版**：给你一套最短命令清单，从 0 到产出全部报告  
3. **论文版**：把代码映射成“方法章节 + 实验章节”的写作提纲

