# EMNLP 投稿补齐清单与脚本实现顺序

## 1) 补齐清单（按投稿必要性排序）

## P0（必须补齐）

1. **检索指标补齐**  
   - 新增并统一报告：`Recall@10`、`Recall@100`、`NDCG@10`、`CCR@10`。  
   - 当前主要是 `CCR@3`，需要升级为论文主口径 `@10`。

2. **基线补齐（公平对比）**  
   - 加入 `BM25` 与 `Cross-Encoder`。  
   - 统一候选池（如 top-100）与评测集，避免不公平对比。

3. **端到端指标补齐**  
   - 增加 `QA Faithfulness`。  
   - 增加 `Negation Failure Rate`。

4. **成本/延迟评估**  
   - 报告 `Vanilla` / `Dual` / `Cross-Encoder` 的 `P50/P95 latency` 与成本。

## P1（强烈建议）

5. **失败案例报告（至少 20 条）**  
   - 按 `negation` / `exclusion` / `numeric` 分桶分析。

6. **真实分布评测集增强**  
   - 在模板数据外增加 300~1000 条真实或半真实 query。

7. **消融实验**  
   - `alpha` only、`tau` only、不加阈值、不同 backbone 等。

---

## 2) 脚本实现顺序（建议按 8 步执行）

1. `experiments/metrics.py`（新建）  
   - 实现通用指标：`recall_at_k`、`ndcg_at_k`、`ccr_at_k`。

2. `experiments/build_retrieval_benchmark.py`（新建）  
   - 统一评测格式：每个 query 提供 `relevant_doc_ids` 与 `constraint_satisfy_labels`。

3. `experiments/eval_retrieval_metrics.py`（新建）  
   - 输出 `Recall@10/100`、`NDCG@10`、`CCR@10`。  
   - 对比 `Vanilla` 与 `Dual`。

4. `experiments/run_bm25_baseline.py`（新建）  
   - 在同一评测集跑 BM25，导出同格式报告。

5. `experiments/run_cross_encoder_baseline.py`（新建）  
   - 在同一候选池 rerank，报告质量上界与时延。

6. `experiments/rag_e2e_eval.py`（新建）  
   - 做端到端评估：Faithfulness 与 Negation Failure Rate。

7. `experiments/latency_benchmark.py`（新建）  
   - 测试 `Vanilla` / `Dual` / `Cross-Encoder` 的 P50/P95。

8. `experiments/error_analysis_report.py`（新建）  
   - 自动生成错误样本报告，至少 20 条。

---

## 3) 每一步的验收门槛

1. `eval_retrieval_metrics.py` 产物必须包含：  
   - `Recall@10`、`Recall@100`、`NDCG@10`、`CCR@10`

2. `BM25` 与 `Cross-Encoder` 必须使用：  
   - 同一评测集  
   - 同一候选池（对 rerank 类方法）

3. `rag_e2e_eval.py` 至少评估 100 条 query  
   - 否则端到端指标方差过大。

4. `latency_benchmark.py` 至少输出：  
   - `P50`、`P95`  
   - 硬件与 batch 设置说明

5. `error_analysis_report.py` 至少输出：  
   - 20 条失败案例  
   - 每条案例有 query、top-k、错误类型、简要原因

---

## 4) 推荐执行节奏（最短路径）

**Day 1**
- 完成 `metrics.py` + `eval_retrieval_metrics.py`
- 先拿到第一版论文主表（Vanilla vs Dual）

**Day 2**
- 完成 `run_bm25_baseline.py`
- 将 BM25 加入主表

**Day 3-4**
- 完成 `run_cross_encoder_baseline.py` + `latency_benchmark.py`
- 拿到质量-延迟对比图/表

**Day 5-6**
- 完成 `rag_e2e_eval.py` + `error_analysis_report.py`
- 形成端到端证据与错误分析章节

---

## 5) 当前状态与缺口对应关系

- 已完成：双视角框架、constraint encoder 训练、网格搜索、分类型 CCR。  
- 待补齐：`Recall/NDCG`、BM25/Cross-Encoder、公平对比、端到端指标、延迟表、系统化误例报告。  

以上补齐后，实验部分会从“可运行验证”提升到“投稿级证据链”。

---

## 6) 已执行进度（最新）

### 已补齐并已跑通

1. `metrics.py`：完成  
2. `build_retrieval_benchmark.py`：完成  
3. `eval_retrieval_metrics.py`：完成并产出  
4. `run_bm25_baseline.py`：完成并产出  
5. `run_cross_encoder_baseline.py`：完成并产出（使用本地缓存的 `cross-encoder/nli-roberta-base`）  
6. `rag_e2e_eval.py`：完成并产出（proxy 版本）  
7. `latency_benchmark.py`：完成并产出  
8. `error_analysis_report.py`：完成并产出（20 条）

### 新增关键报告

- `outputs/reports/retrieval_metrics_local_dual.json`
- `outputs/reports/bm25_metrics_report.json`
- `outputs/reports/cross_encoder_metrics_report.json`
- `outputs/reports/rag_e2e_proxy_dual.json`
- `outputs/reports/rag_e2e_proxy_vanilla.json`
- `outputs/reports/latency_benchmark_report_with_cross.json`
- `outputs/reports/error_analysis_cases.md`

### 当前主要结果（同一检索评测集）

- **Dual vs Vanilla（Local Llama topic）**  
  - Recall@10: `0.6617 -> 0.7178`  
  - NDCG@10: `0.6533 -> 0.7230`  
  - CCR@10: `0.1987 -> 0.2323`

- **BM25（Baseline）**  
  - Recall@10: `0.5456`  
  - NDCG@10: `0.5421`  
  - CCR@10: `0.0863`

- **Cross-Encoder（NLI-RoBERTa 本地缓存）**  
  - Recall@10: `0.4067`  
  - NDCG@10: `0.3804`  
  - CCR@10: `0.1660`

### 注意事项（评测解释边界）

- 目前 cross-encoder 选的是 NLI 模型，非专门 IR reranker，因此结果偏弱是合理现象。  
- `rag_e2e_eval.py` 当前是 **proxy faithfulness**（基于 top1 约束满足）而不是完整 LLM 打分器。  
- `latency_benchmark_report_with_cross.json` 里的 cross-encoder 时间主要是 rerank 步骤时间，不含完整检索链路全部开销。

